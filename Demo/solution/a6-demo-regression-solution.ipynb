{"cells":[{"metadata":{"_uuid":"74fb99ef4e84ce85fefa1000051127a5f667bed5"},"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n## Open Machine Learning Course\n<center>Author: [Yury Kashnitsky](https://www.linkedin.com/in/festline/), Data Scientist at Mail.ru Group <br>\n    All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.\nYou may use this material for any purpose (you can edit, correct and use it as example) exept commercial use with mandatory citation of author."},{"metadata":{"_uuid":"03f05b56b239aadafbb0a3d71f4e546440371089"},"cell_type":"markdown","source":"# <center> Assignment #6 (demo).\n## <center>  Exploring OLS, Lasso and Random Forest in a regression task\n    \n<img src=\"https://habrastorage.org/webt/-h/ns/aa/-hnsaaifymavmmudwip9imcmk58.jpeg\" width=30%>\n\n**Fill in the missing code and choose answers in [this](https://docs.google.com/forms/d/1aHyK58W6oQmNaqEfvpLTpo6Cb0-ntnvJ18rZcvclkvw/edit) web form.**"},{"metadata":{"trusted":false,"_uuid":"390adbc8ccbbc54742414e51dea164d6bd5be20f"},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics.regression import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LinearRegression, LassoCV, Lasso\nfrom sklearn.ensemble import RandomForestRegressor","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d62320064b9f970e807a87ea86920f717fdbcacc"},"cell_type":"markdown","source":"**We are working with UCI Wine quality dataset (no need to download it â€“ it's already there, in course repo and in Kaggle Dataset).**"},{"metadata":{"trusted":false,"_uuid":"6a1866d55a303649cfdacda97d388f7bc749d5a5"},"cell_type":"code","source":"data = pd.read_csv('../input/winequality-white.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"a7cca38c9ea33b41c0321620181461abe2536f50"},"cell_type":"code","source":"data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"e65de40b617a353ccb764605c9a3ee8b6ff2f02a"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d71b8f450b1d429264f32dd62b28d798fed79128"},"cell_type":"markdown","source":"**Separate the target feature, split data in 7:3 proportion (30% form a holdout set, use random_state=17), and preprocess data with `StandardScaler`.**"},{"metadata":{"trusted":false,"_uuid":"1094851ed35bf323ed5c32917f5645088087e1bc"},"cell_type":"code","source":"y = data['quality']\nX = data.drop('quality', axis=1)\n\nX_train, X_holdout, y_train, y_holdout = train_test_split(X, y, test_size=0.3, \n                                                          random_state=17)\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_holdout_scaled = scaler.transform(X_holdout)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3ddf4bc6aa92507ec6838af08f439d7d700b137c"},"cell_type":"markdown","source":"## Linear regression"},{"metadata":{"_uuid":"84a1b17ac802aac0b58d97024c5af58cd79a3a5a"},"cell_type":"markdown","source":"**Train a simple linear regression model (Ordinary Least Squares).**"},{"metadata":{"trusted":false,"_uuid":"b53a91cc10a46f6f518de15e5d3d0aa3668c1bdb"},"cell_type":"code","source":"linreg = LinearRegression()\nlinreg.fit(X_train_scaled, y_train);","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9b785a3c4502395cd52b139f4f7ac68247f85663"},"cell_type":"markdown","source":"**<font color='red'>Question 1:</font> What are mean squared errors of model predictions on train and holdout sets?**"},{"metadata":{"trusted":false,"_uuid":"f7cf7b2ffd021173c5af60dc3bc6dfc1a228ac18"},"cell_type":"code","source":"print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, linreg.predict(X_train_scaled)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, linreg.predict(X_holdout_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e86136a1bfeee73b483d3c0165465b2a4cd3aec"},"cell_type":"markdown","source":"**Sort features by their influence on the target feature (wine quality). Beware that both large positive and large negative coefficients mean large influence on target. It's handy to use `pandas.DataFrame` here.**\n\n**<font color='red'>Question 2:</font> Which feature this linear regression model treats as the most influential on wine quality?**"},{"metadata":{"trusted":false,"_uuid":"88f9b68f1d475934ffe85ee67145aca6f9617c82"},"cell_type":"code","source":"linreg_coef = pd.DataFrame({'coef': linreg.coef_, 'coef_abs': np.abs(linreg.coef_)},\n                          index=data.columns.drop('quality'))\nlinreg_coef.sort_values(by='coef_abs', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"fe5ad0a4a0dbf0021aa571589e3a65ffce4be7da"},"cell_type":"markdown","source":"## Lasso regression"},{"metadata":{"_uuid":"8a9ca268c103a835a574a40fe49d8a08792da5b2"},"cell_type":"markdown","source":"**Train a LASSO model with $\\alpha = 0.01$ (weak regularization) and scaled data. Again, set random_state=17.**"},{"metadata":{"trusted":false,"_uuid":"6a1379644fd83838b1589853e6a8011b58420f19"},"cell_type":"code","source":"lasso1 = Lasso(alpha=0.01, random_state=17)\nlasso1.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"955ff9a06db2ad8d9bddfb75f3b6cfff515e090d"},"cell_type":"markdown","source":"**Which feature is the least informative in predicting wine quality, according to this LASSO model?**"},{"metadata":{"trusted":false,"_uuid":"d8af94e687cda53240b8c3430e2a3c1fb9366df2"},"cell_type":"code","source":"lasso1_coef = pd.DataFrame({'coef': lasso1.coef_, 'coef_abs': np.abs(lasso1.coef_)},\n                          index=data.columns.drop('quality'))\nlasso1_coef.sort_values(by='coef_abs', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d748b92eda558d669fbfee2a7ce25d4652e374fd"},"cell_type":"markdown","source":"**Train LassoCV with random_state=17 to choose the best value of $\\alpha$ in 5-fold cross-validation.**"},{"metadata":{"trusted":false,"_uuid":"5369e8913e6c2dc5c127049d14841cbc01283d52"},"cell_type":"code","source":"alphas = np.logspace(-6, 2, 200)\nlasso_cv = LassoCV(random_state=17, cv=5, alphas=alphas)\nlasso_cv.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"8712543ffb694dca9518ca7e04e90a9c668e4a22"},"cell_type":"code","source":"lasso_cv.alpha_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15f4f41be090815db74e461f5f6ea6a414efce85"},"cell_type":"markdown","source":"**<font color='red'>Question 3:</font> Which feature is the least informative in predicting wine quality, according to the tuned LASSO model?**"},{"metadata":{"trusted":false,"_uuid":"6588d6826b3a73010e5b50771d013a6cca7b11fa"},"cell_type":"code","source":"lasso_cv_coef = pd.DataFrame({'coef': lasso_cv.coef_, 'coef_abs': np.abs(lasso_cv.coef_)},\n                          index=data.columns.drop('quality'))\nlasso_cv_coef.sort_values(by='coef_abs', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7ba5cdc3394fd82fa2c6f36fc8d5685d70847edf"},"cell_type":"markdown","source":"**<font color='red'>Question 4:</font> What are mean squared errors of tuned LASSO predictions on train and holdout sets?**"},{"metadata":{"trusted":false,"_uuid":"909b5627ba062e7f2dc200ae4e65972f33650eda"},"cell_type":"code","source":"print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, lasso_cv.predict(X_train_scaled)))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, lasso_cv.predict(X_holdout_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f34cf1a5d4cddbe423349ce2057a9abe3ad09ff5"},"cell_type":"markdown","source":"## Random Forest"},{"metadata":{"_uuid":"14caef9e5be0fc204848fc8053db2e98d5c6fbad"},"cell_type":"markdown","source":"**Train a Random Forest with out-of-the-box parameters, setting only random_state to be 17.**"},{"metadata":{"trusted":false,"_uuid":"34436e29fb5fe6c507c6fc46639c45f626f1dc0f"},"cell_type":"code","source":"forest = RandomForestRegressor(random_state=17)\nforest.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"751fbb75d0f350aac2bc45dc01f8a8cfd55c19c8"},"cell_type":"markdown","source":"**<font color='red'>Question 5:</font> What are mean squared errors of RF model on the training set, in cross-validation (cross_val_score with scoring='neg_mean_squared_error' and other arguments left with default values) and on holdout set?**"},{"metadata":{"trusted":false,"_uuid":"35974f6472e76e142068aeeb27c1073d0251351f"},"cell_type":"code","source":"print(\"Mean squared error (train): %.3f\" % mean_squared_error(y_train, forest.predict(X_train_scaled)))\nprint(\"Mean squared error (cv): %.3f\" % np.mean(np.abs(cross_val_score(forest, X_train_scaled, y_train, \n                                                                       scoring='neg_mean_squared_error'))))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, forest.predict(X_holdout_scaled)))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b83bd48b8e482e978ec3059d132477909647f5c3"},"cell_type":"markdown","source":"**Tune the `max_features` and `max_depth` hyperparameters with GridSearchCV and again check mean cross-validation MSE and MSE on holdout set.**"},{"metadata":{"trusted":false,"_uuid":"f9e2578d40f11ea545bfe8755c1d395a77fdaf61"},"cell_type":"code","source":"forest_params = {'max_depth': list(range(10, 25)), \n                  'max_features': list(range(6,12))}\n\nlocally_best_forest = GridSearchCV(RandomForestRegressor(n_jobs=-1, random_state=17), \n                                 forest_params, \n                                 scoring='neg_mean_squared_error',  \n                                 n_jobs=-1, cv=5,\n                                  verbose=True)\nlocally_best_forest.fit(X_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false,"_uuid":"1749084c8361cb009159eb4253790fd43e19970b"},"cell_type":"code","source":"locally_best_forest.best_params_, locally_best_forest.best_score_","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5451e6b3252142202c3a7eee5822a868901a3776"},"cell_type":"markdown","source":"**<font color='red'>Question 6:</font> What are mean squared errors of tuned RF model in cross-validation (cross_val_score with scoring='neg_mean_squared_error' and other arguments left with default values) and on holdout set?**"},{"metadata":{"trusted":false,"_uuid":"d771ef1d7be5c7bc8c6230b79a95bc45675c8a75"},"cell_type":"code","source":"print(\"Mean squared error (cv): %.3f\" % np.mean(np.abs(cross_val_score(locally_best_forest.best_estimator_,\n                                                        X_train_scaled, y_train, \n                                                        scoring='neg_mean_squared_error'))))\nprint(\"Mean squared error (test): %.3f\" % mean_squared_error(y_holdout, \n                                                             locally_best_forest.predict(X_holdout_scaled)))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"40f853707d682b649e64755c81d40e09cd01c1ca"},"cell_type":"markdown","source":"**Output RF's feature importance. Again, it's nice to present it as a DataFrame.**<br>\n**<font color='red'>Question 7:</font> What is the most important feature, according to the Random Forest model?**"},{"metadata":{"trusted":false,"_uuid":"af55cf361e4764ba213fc70f4170ff49336e23e6"},"cell_type":"code","source":"rf_importance = pd.DataFrame(locally_best_forest.best_estimator_.feature_importances_, \n                             columns=['coef'], index=data.columns[:-1]) \nrf_importance.sort_values(by='coef', ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b2595c4fb42c89c2bb9f2acfb3a7b85cda3759b3"},"cell_type":"markdown","source":"**Make conclusions about the perdormance of the explored 3 models in this particular prediction task.**\n\nThe depency of wine quality on other features in hand is, presumable, non-linear. So Random Forest works better in this task. "}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"name":"lesson8_part1_kmeans.ipynb"},"nbformat":4,"nbformat_minor":1}